{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#固有表現抽出は、テキストに出現する人名や地名などの固有名詞や、日付や時間などの数値表現を抽出する技術です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#必要なPythonモジュール\n",
    "#pip install numpy\n",
    "#pip install scipy\n",
    "#pip install sklearn\n",
    "#pip install python-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pycrfsuite\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "class CorpusReader(object):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        with codecs.open(path, encoding='utf-8') as f:\n",
    "            sent = []\n",
    "            sents = []\n",
    "            for line in f:\n",
    "                if line == '\\n':\n",
    "                    sents.append(sent)\n",
    "                    sent = []\n",
    "                    continue\n",
    "                morph_info = line.strip().split('\\t')\n",
    "                sent.append(morph_info)\n",
    "        train_num = int(len(sents) * 0.9)\n",
    "        self.__train_sents = sents[:train_num]\n",
    "        self.__test_sents = sents[train_num:]\n",
    "\n",
    "    def iob_sents(self, name):\n",
    "        if name == 'train':\n",
    "            return self.__train_sents\n",
    "        elif name == 'test':\n",
    "            return self.__test_sents\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CorpusReader('hironsan.txt')\n",
    "train_sents = c.iob_sents('train')\n",
    "test_sents = c.iob_sents('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2005', '名詞', '数', '*', '*', '*', '*', '*', 'B-DAT'],\n",
       " ['年', '名詞', '接尾', '助数詞', '*', '*', '*', '年', 'ネン', 'ネン', 'I-DAT'],\n",
       " ['7', '名詞', '数', '*', '*', '*', '*', '*', 'I-DAT'],\n",
       " ['月', '名詞', '一般', '*', '*', '*', '*', '月', 'ツキ', 'ツキ', 'I-DAT'],\n",
       " ['14', '名詞', '数', '*', '*', '*', '*', '*', 'I-DAT'],\n",
       " ['日', '名詞', '接尾', '助数詞', '*', '*', '*', '日', 'ニチ', 'ニチ', 'I-DAT'],\n",
       " ['、', '記号', '読点', '*', '*', '*', '*', '、', '、', '、', 'O'],\n",
       " ['南アフリカ',\n",
       "  '名詞',\n",
       "  '固有名詞',\n",
       "  '地域',\n",
       "  '国',\n",
       "  '*',\n",
       "  '*',\n",
       "  '南アフリカ',\n",
       "  'ミナミアフリカ',\n",
       "  'ミナミアフリカ',\n",
       "  'B-LOC'],\n",
       " ['共和', '名詞', '一般', '*', '*', '*', '*', '共和', 'キョウワ', 'キョーワ', 'I-LOC'],\n",
       " ['国', '名詞', '接尾', '一般', '*', '*', '*', '国', 'コク', 'コク', 'I-LOC'],\n",
       " ['ダーバン', '名詞', '固有名詞', '地域', '一般', '*', '*', 'ダーバン', 'ダーバン', 'ダーバン', 'I-LOC'],\n",
       " ['で', '助詞', '格助詞', '一般', '*', '*', '*', 'で', 'デ', 'デ', 'O'],\n",
       " ['開催', '名詞', 'サ変接続', '*', '*', '*', '*', '開催', 'カイサイ', 'カイサイ', 'O'],\n",
       " ['中', '名詞', '接尾', '副詞可能', '*', '*', '*', '中', 'チュウ', 'チュー', 'O'],\n",
       " ['の', '助詞', '連体化', '*', '*', '*', '*', 'の', 'ノ', 'ノ', 'O'],\n",
       " ['第', '接頭詞', '数接続', '*', '*', '*', '*', '第', 'ダイ', 'ダイ', 'O'],\n",
       " ['29', '名詞', '数', '*', '*', '*', '*', '*', 'O'],\n",
       " ['回', '名詞', '接尾', '助数詞', '*', '*', '*', '回', 'カイ', 'カイ', 'O'],\n",
       " ['世界', '名詞', '一般', '*', '*', '*', '*', '世界', 'セカイ', 'セカイ', 'B-ORG'],\n",
       " ['遺産', '名詞', '一般', '*', '*', '*', '*', '遺産', 'イサン', 'イサン', 'I-ORG'],\n",
       " ['会議', '名詞', 'サ変接続', '*', '*', '*', '*', '会議', 'カイギ', 'カイギ', 'I-ORG'],\n",
       " ['で', '助詞', '格助詞', '一般', '*', '*', '*', 'で', 'デ', 'デ', 'O'],\n",
       " ['、', '記号', '読点', '*', '*', '*', '*', '、', '、', '、', 'O'],\n",
       " ['ノルウェー',\n",
       "  '名詞',\n",
       "  '固有名詞',\n",
       "  '地域',\n",
       "  '国',\n",
       "  '*',\n",
       "  '*',\n",
       "  'ノルウェー',\n",
       "  'ノルウェー',\n",
       "  'ノルウェー',\n",
       "  'B-LOC'],\n",
       " ['の', '助詞', '連体化', '*', '*', '*', '*', 'の', 'ノ', 'ノ', 'O'],\n",
       " ['フィヨルド', '名詞', '一般', '*', '*', '*', '*', 'フィヨルド', 'フィヨルド', 'フィヨルド', 'O'],\n",
       " ['と', '助詞', '並立助詞', '*', '*', '*', '*', 'と', 'ト', 'ト', 'O'],\n",
       " ['日本', '名詞', '固有名詞', '地域', '国', '*', '*', '日本', 'ニッポン', 'ニッポン', 'B-LOC'],\n",
       " ['の', '助詞', '連体化', '*', '*', '*', '*', 'の', 'ノ', 'ノ', 'O'],\n",
       " ['知床', '名詞', '固有名詞', '地域', '一般', '*', '*', '知床', 'シレトコ', 'シレトコ', 'B-LOC'],\n",
       " ['半島', '名詞', '一般', '*', '*', '*', '*', '半島', 'ハントウ', 'ハントー', 'I-LOC'],\n",
       " ['（', '記号', '括弧開', '*', '*', '*', '*', '（', '（', '（', 'O'],\n",
       " ['北海道',\n",
       "  '名詞',\n",
       "  '固有名詞',\n",
       "  '地域',\n",
       "  '一般',\n",
       "  '*',\n",
       "  '*',\n",
       "  '北海道',\n",
       "  'ホッカイドウ',\n",
       "  'ホッカイドー',\n",
       "  'B-LOC'],\n",
       " ['）', '記号', '括弧閉', '*', '*', '*', '*', '）', '）', '）', 'O'],\n",
       " ['含め', '動詞', '自立', '*', '*', '一段', '連用形', '含める', 'フクメ', 'フクメ', 'O'],\n",
       " ['世界', '名詞', '一般', '*', '*', '*', '*', '世界', 'セカイ', 'セカイ', 'O'],\n",
       " ['7', '名詞', '数', '*', '*', '*', '*', '*', 'O'],\n",
       " ['カ所', '名詞', '接尾', '助数詞', '*', '*', '*', 'カ所', 'カショ', 'カショ', 'O'],\n",
       " ['が', '助詞', '格助詞', '一般', '*', '*', '*', 'が', 'ガ', 'ガ', 'O'],\n",
       " ['ユネスコ', '名詞', '固有名詞', '組織', '*', '*', '*', 'ユネスコ', 'ユネスコ', 'ユネスコ', 'B-ORG'],\n",
       " ['の', '助詞', '連体化', '*', '*', '*', '*', 'の', 'ノ', 'ノ', 'O'],\n",
       " ['世界', '名詞', '一般', '*', '*', '*', '*', '世界', 'セカイ', 'セカイ', 'O'],\n",
       " ['自然', '名詞', '形容動詞語幹', '*', '*', '*', '*', '自然', 'シゼン', 'シゼン', 'O'],\n",
       " ['遺産', '名詞', '一般', '*', '*', '*', '*', '遺産', 'イサン', 'イサン', 'O'],\n",
       " ['に', '助詞', '格助詞', '一般', '*', '*', '*', 'に', 'ニ', 'ニ', 'O'],\n",
       " ['登録', '名詞', 'サ変接続', '*', '*', '*', '*', '登録', 'トウロク', 'トーロク', 'O'],\n",
       " ['さ', '動詞', '自立', '*', '*', 'サ変・スル', '未然レル接続', 'する', 'サ', 'サ', 'O'],\n",
       " ['れ', '動詞', '接尾', '*', '*', '一段', '連用形', 'れる', 'レ', 'レ', 'O'],\n",
       " ['た', '助動詞', '*', '*', '*', '特殊・タ', '基本形', 'た', 'タ', 'タ', 'O'],\n",
       " ['。', '記号', '句点', '*', '*', '*', '*', '。', '。', '。', 'O']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文字種の判定\n",
    "#文字列に含まれるすべての文字種を-(ハイフン)で結合しています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hiragana(ch):\n",
    "    return 0x3040 <= ord(ch) <= 0x309F\n",
    "\n",
    "def is_katakana(ch):\n",
    "    return 0x30A0 <= ord(ch) <= 0x30FF\n",
    "\n",
    "def get_character_type(ch):\n",
    "    if ch.isspace():\n",
    "        return 'ZSPACE'\n",
    "    elif ch.isdigit():\n",
    "        return 'ZDIGIT'\n",
    "    elif ch.islower():\n",
    "        return 'ZLLET'\n",
    "    elif ch.isupper():\n",
    "        return 'ZULET'\n",
    "    elif is_hiragana(ch):\n",
    "        return 'HIRAG'\n",
    "    elif is_katakana(ch):\n",
    "        return 'KATAK'\n",
    "    else:\n",
    "        return 'OTHER'\n",
    "\n",
    "def get_character_types(string):\n",
    "    character_types = map(get_character_type, string)\n",
    "    character_types_str = '-'.join(sorted(set(character_types)))\n",
    "\n",
    "    return character_types_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#品詞細分類の抽出\n",
    "#形態素情報から品詞細分類を抽出する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos_with_subtype(morph):\n",
    "    idx = morph.index('*')\n",
    "\n",
    "    return '-'.join(morph[1:idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#素性抽出\n",
    "#各単語に対して素性抽出をする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    chtype = get_character_types(sent[i][0])\n",
    "    postag = extract_pos_with_subtype(sent[i])\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word=' + word,\n",
    "        'type=' + chtype,\n",
    "        'postag=' + postag,\n",
    "    ]\n",
    "    if i >= 2:\n",
    "        word2 = sent[i-2][0]\n",
    "        chtype2 = get_character_types(sent[i-2][0])\n",
    "        postag2 = extract_pos_with_subtype(sent[i-2])\n",
    "        iobtag2 = sent[i-2][-1]\n",
    "        features.extend([\n",
    "            '-2:word=' + word2,\n",
    "            '-2:type=' + chtype2,\n",
    "            '-2:postag=' + postag2,\n",
    "            '-2:iobtag=' + iobtag2,\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "\n",
    "    if i >= 1:\n",
    "        word1 = sent[i-1][0]\n",
    "        chtype1 = get_character_types(sent[i-1][0])\n",
    "        postag1 = extract_pos_with_subtype(sent[i-1])\n",
    "        iobtag1 = sent[i-1][-1]\n",
    "        features.extend([\n",
    "            '-1:word=' + word1,\n",
    "            '-1:type=' + chtype1,\n",
    "            '-1:postag=' + postag1,\n",
    "            '-1:iobtag=' + iobtag1,\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        chtype1 = get_character_types(sent[i+1][0])\n",
    "        postag1 = extract_pos_with_subtype(sent[i+1])\n",
    "        features.extend([\n",
    "            '+1:word=' + word1,\n",
    "            '+1:type=' + chtype1,\n",
    "            '+1:postag=' + postag1,\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "\n",
    "    if i < len(sent)-2:\n",
    "        word2 = sent[i+2][0]\n",
    "        chtype2 = get_character_types(sent[i+2][0])\n",
    "        postag2 = extract_pos_with_subtype(sent[i+2])\n",
    "        features.extend([\n",
    "            '+2:word=' + word2,\n",
    "            '+2:type=' + chtype2,\n",
    "            '+2:postag=' + postag2,\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [morph[-1] for morph in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [morph[0] for morph in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent2featuresで文から素性を抽出します。実際に抽出される素性は以下のようになります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias',\n",
       " 'word=2005',\n",
       " 'type=ZDIGIT',\n",
       " 'postag=名詞-数',\n",
       " 'BOS',\n",
       " 'BOS',\n",
       " '+1:word=年',\n",
       " '+1:type=OTHER',\n",
       " '+1:postag=名詞-接尾-助数詞',\n",
       " '+2:word=7',\n",
       " '+2:type=ZDIGIT',\n",
       " '+2:postag=名詞-数']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> sent2features(train_sents[0])[0]\n",
    "['bias',\n",
    " 'word=2005',\n",
    " 'type=ZDIGIT',\n",
    " 'postag=名詞-数',\n",
    " 'BOS',\n",
    " 'BOS',\n",
    " '+1:word=年',\n",
    " '+1:type=OTHER',\n",
    " '+1:postag=名詞-接尾-助数詞',\n",
    " '+2:word=7',\n",
    " '+2:type=ZDIGIT',\n",
    " '+2:postag=名詞-数']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルを学習するために、pycrfsuite.Trainerオブジェクトを作成し、学習データを読み込ませた後、trainメソッドを呼び出します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pycrfsuite' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\github_file\\japanese_gammer_checker\\koyouhyokenn.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/github_file/japanese_gammer_checker/koyouhyokenn.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m pycrfsuite\u001b[39m.\u001b[39mTrainer(verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/github_file/japanese_gammer_checker/koyouhyokenn.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m xseq, yseq \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(X_train, y_train):\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/github_file/japanese_gammer_checker/koyouhyokenn.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     trainer\u001b[39m.\u001b[39mappend(xseq, yseq)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pycrfsuite' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#次に学習パラメータを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train('model.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習したモデルを使用するためには、pycrfsuite.Taggerオブジェクトを作成し、学習したモデルを読み込み、tagメソッドを使用します。\n",
    "#まずは、Taggerオブジェクトの作成と学習済みモデルの読み込みを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x183b745bec8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('model.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "昨年 10 月 に は 、 34 人 が 、 今回 の 現場 に 近い エジプト の タバ で 爆発 事件 の ため 死亡 し て いる 。\n",
      "Predicted: B-DAT I-DAT I-DAT O O O O O O O O O O O O B-LOC O B-LOC O O O O O O O O O O\n",
      "Correct:   B-DAT I-DAT I-DAT O O O O O O O O O O O O B-LOC O B-LOC O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "example_sent = test_sents[0]\n",
    "print(' '.join(sent2tokens(example_sent)))\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
    "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルの評価\n",
    "#作成したモデルについて評価していきましょう。評価は、適合率、再現率、F値で行います。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_classification_report(y_true, y_pred):\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "\n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "\n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#評価に使うためのテストデータ集合内の文に対してタグ付けします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [tagger.tag(xseq) for xseq in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習したモデルを用いてタグ付けしたデータと、正解データを評価用の関数に渡して結果を表示します。\n",
    "#各カテゴリについて、適合率、再現率、F値、タグ数を表示しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ART       1.00      0.89      0.94         9\n",
      "       I-ART       0.92      1.00      0.96        12\n",
      "       B-DAT       1.00      1.00      1.00        12\n",
      "       I-DAT       1.00      1.00      1.00        22\n",
      "       B-LOC       1.00      0.95      0.97        55\n",
      "       I-LOC       0.94      0.94      0.94        17\n",
      "       B-ORG       0.75      0.86      0.80        14\n",
      "       I-ORG       1.00      0.90      0.95        10\n",
      "       B-PSN       0.00      0.00      0.00         3\n",
      "       B-TIM       1.00      0.71      0.83         7\n",
      "       I-TIM       1.00      0.81      0.90        16\n",
      "\n",
      "   micro avg       0.96      0.91      0.94       177\n",
      "   macro avg       0.87      0.82      0.84       177\n",
      "weighted avg       0.95      0.91      0.93       177\n",
      " samples avg       0.14      0.14      0.14       177\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tf2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\envs\\tf2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\envs\\tf2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    ">>> print(bio_classification_report(y_test, y_pred))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d076c9a21c6d20f2499ac2f833e02391f3071b5fe0ead22b97b4b85f484c4364"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
